---
layout: post
title: "精排模型分类损失加入辅助排序损失"
date: 2024-11-03
categories: blogging
---
近期在调研LTR建模时，关注到业界现在的主流方案倾向于使用BCE与LTR结合建模的方式。

相关工作有YoutubeSerach的RCR、阿里的JRC等方法。此外，腾讯广告团队也对分类损失额外加入辅助排序损失，反而在分类Loss、分类效果上均有提升的现象，从梯度消失的视角做出了分析。

本文将对调研过程中了解到的相关工作进行简单梳理和总结。

## 2023KDD - RCR

[Regression Compatible Listwise Objectives for Calibrated Ranking with Binary Relevance](https://arxiv.org/pdf/2211.01494)

论文分析了ListNet中提出的损失函数

$$
L^{MultiObj}_{query}(\theta;q)=(1-\alpha)\cdot\sum_{i}^{N}\text{SigmoidCE}(s_i,y_i)+\alpha\cdot\text{SoftmaxCE}(s_{1:N},y_{1_N})\\
\text{SigmoidCE}(s_i,y_i) = -y\cdot log\sigma(s)-(1-y)\cdot log(1-\sigma(s))\\
\text{SoftmaxCE}(s_{1:N},y_{1:N})=\frac{1}{N}\sum_{i}^{N}y_i\cdot log\frac{\text{exp}(s_i)}{\sum_{j}^{N}\text{exp}(s_j)}
$$

其中，$\text{SigmoidCE}$的优化目标是$\sigma(s_i)\rightarrow P_i$，即

$$
s_i\rightarrow logP_i-log(1-P_i)

$$

$\text{SoftmaxCE}$的优化目标是

$$
\frac{\text{exp}(s_i)}{\sum_{j}^{N}\text{exp}(s_j)}\rightarrow\frac{P_i}{\sum_j^N P_j}，
$$

即：

$$
s_i \rightarrow logP_i - log\sum_j^NP_j+log\sum_j^N\text{exp}(s_j)
$$

通过上述分析可知，两个loss对$s$的优化目标并不一致，因此两个loss会导致$s$最终收敛到次优解。

本文提出的$\text{ListCE}$，修改了listwise部分的计算，从而在优化目标层面上，达到：

1. $\text{listCE}$优化的序，与$\text{SoftmaxCE}$对齐；
2. $\text{listCE}$优化的分值$s$，与$\text{SigmoidCE}$对齐。
