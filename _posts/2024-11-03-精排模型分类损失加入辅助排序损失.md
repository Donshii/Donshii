---
layout: post
title: "精排模型分类损失加入辅助排序损失"
date: 2024-11-03
categories: blogging
---
近期在调研LTR建模时，关注到业界现在的主流方案倾向于使用BCE与LTR结合建模的方式。

相关工作有YoutubeSerach的RCR、阿里的JRC等方法。此外，腾讯广告团队也对分类损失额外加入辅助排序损失，反而在分类Loss、分类效果上均有提升的现象，从梯度消失的视角做出了分析。

本文将对调研过程中了解到的相关工作进行简单梳理和总结。

## 2023KDD - RCR

[Regression Compatible Listwise Objectives for Calibrated Ranking with Binary Relevance](https://arxiv.org/pdf/2211.01494)

### 问题背景：排序损失的优化目标和分类损失的优化目标不对齐

论文分析了ListNet中提出的损失函数

$$
L^{MultiObj}_{query}(\theta;q)=(1-\alpha)\cdot\sum_{i}^{N}\text{SigmoidCE}(s_i,y_i)+\alpha\cdot\text{SoftmaxCE}(s_{1:N},y_{1_N})\tag{1}
$$

$$
\text{SigmoidCE}(s_i,y_i) = -y\cdot log\sigma(s)-(1-y)\cdot log(1-\sigma(s))\tag{2}
$$

$$
\text{SoftmaxCE}(s_{1:N},y_{1:N})=\frac{1}{N}\sum_{i}^{N}y_i\cdot log\frac{\text{exp}(s_i)}{\sum_{j}^{N}\text{exp}(s_j)}\tag{3}
$$

其中，$\text{SigmoidCE}$的优化目标是

$$
\sigma(s_i)\rightarrow P_i\tag{4},
$$

即

$$
s_i\rightarrow logP_i-log(1-P_i)\tag{5}
$$

$\text{SoftmaxCE}$的优化目标是

$$
\frac{\text{exp}(s_i)}{\sum_{j}^{N}\text{exp}(s_j)}\rightarrow\frac{P_i}{\sum_j^N P_j}\tag{6}，
$$

即：

$$
s_i \rightarrow logP_i - log\sum_j^NP_j+log\sum_j^N\text{exp}(s_j)\tag{7}
$$

通过上述分析可知，两个loss对$s$的优化目标并不一致，因此两个loss会导致$s$最终收敛到次优解。

本文提出的$\text{ListCE}$，修改了listwise部分的计算，从而在优化目标层面上，达到：

1. $\text{listCE}$优化的序，与$\text{SoftmaxCE}$对齐；
2. $\text{listCE}$优化的分值$s$，与$\text{SigmoidCE}$对齐。

### $\text{ListCE}$函数提出与分析（「序」与「分」）

#### 序

论文提出对原排序损失的改造，提出任意一个保序的函数$\text{T(s)}$形式的损失函数$\text{ListCE}$：

$$
\text{ListCE}(T,s_{1:N},y_{1:N})=\frac{1}{C}\sum_{i}^{N}y_i\cdot log\frac{\text{T}(s_i)}{\sum_{j}^{N}\text{T}(s_j)}\tag{8}
$$

优化目标和原始的$\text{exp(s)}$的优化目标，是同一个**「序」**

具体推导上，使用拉格朗日算子法，得到其解为:

$$
\frac{\text{T}(s_i)}{\sum_{j}^{N}\text{T}(s_j)}\rightarrow \frac{\mathbb{E}[y_{i}|q, x_i ]}{\sum_j^N\mathbb{E}[y_j|q,x_j ]} \tag{9}
$$

式$(9)$和式$(6)$的收敛目标是同一个收敛目标，左侧函数$\text{T(s)}$和$\text{exp}(s)$都是保序函数，因此式$(9)$和式$(6)$的优化目标是同一个**「序」**

#### 分

根据瞪眼法，令$\text{T}(s)=\sigma(s)$时，式$(9)$改写为

$$
\frac{\sigma(s_i)}{\sum_{j}^{N}\sigma(s_j)}\rightarrow \frac{\mathbb{E}[y_{i}|q, x_i ]}{\sum_j^N\mathbb{E}[y_j|q,x_j ]} \tag{10}
$$

式$(4)$的优化目标，是式$(10)$解集上的一个解。因此作者认为在分值优化上，新的$\text{ListCE}$损失函数不会把分类目标$\text{SoftmaxCE}$带偏。



## 2024 - KDD - **深入理解推荐模型中的辅助排序损失**

[Understanding the Ranking Loss for Recommendation with Sparse User Feedback](https://arxiv.org/pdf/2403.14144)

本文在梯度消失的视角讨论了为什么引入辅助排序损失是有效的

**结论1：引入辅助损失函数，在验证集上获得了比BCE方法更低的BCE损失，表明它提高了分类能力，而不仅仅是排序能力。**

**结论2：引入辅助损失函数，在训练集上获得了比BCE方法更低的BCE损失，表明引入辅助排序损失有助于BCE损失的优化。**

### 梯度消失
